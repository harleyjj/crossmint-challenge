name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12']
        exclude:
          # Exclude some combinations to speed up CI
          - os: macos-latest
            python-version: '3.8'
          - os: windows-latest
            python-version: '3.8'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Lint with flake8
      run: |
        # Install flake8 if not in requirements
        pip install flake8
        # Stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Type checking with mypy
      run: |
        # Run type checking
        mypy src/ --ignore-missing-imports --no-strict-optional
      continue-on-error: true  # Don't fail CI on type errors for now

    - name: Test with pytest
      run: |
        python -m pytest tests/ -v --tb=short
      env:
        # Set test environment variables
        CANDIDATE_ID: test-candidate-id
        REQUEST_DELAY: 0.1
        MAX_RETRIES: 2
        GOAL_FILE: tests/fixtures/sample_goal.json
        LOG_LEVEL: DEBUG

    - name: Test with coverage
      run: |
        python -m pytest tests/ --cov=src --cov-report=xml --cov-report=html --cov-report=term-missing
      env:
        CANDIDATE_ID: test-candidate-id
        REQUEST_DELAY: 0.1
        MAX_RETRIES: 2
        GOAL_FILE: tests/fixtures/sample_goal.json
        LOG_LEVEL: DEBUG

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  security-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install safety bandit

    - name: Security audit with safety
      run: |
        safety check

    - name: Security scan with bandit
      run: |
        bandit -r src/ -f json -o bandit-report.json
      continue-on-error: true

    - name: Upload bandit report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: bandit-report
        path: bandit-report.json

  integration-test:
    runs-on: ubuntu-latest
    needs: test
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run integration tests
      run: |
        python -m pytest tests/integration/ -v --tb=short
      env:
        CANDIDATE_ID: test-candidate-id
        REQUEST_DELAY: 0.1
        MAX_RETRIES: 2
        GOAL_FILE: tests/fixtures/sample_goal.json
        LOG_LEVEL: DEBUG

    - name: Test CLI commands
      run: |
        # Test preview command (should not make API calls)
        python main.py --log-level DEBUG preview

        # Test help command
        python main.py --help

        # Test create command (dry run with test data)
        echo "Testing CLI functionality..."
      env:
        CANDIDATE_ID: test-candidate-id
        REQUEST_DELAY: 0.1
        MAX_RETRIES: 2
        GOAL_FILE: tests/fixtures/sample_goal.json
        LOG_LEVEL: DEBUG

  build:
    runs-on: ubuntu-latest
    needs: [test, security-scan]
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build

    - name: Build package
      run: |
        python -m build

    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: dist-packages
        path: dist/

  notify:
    runs-on: ubuntu-latest
    needs: [test, security-scan, integration-test]
    if: always()
    steps:
    - name: Notify on success
      if: needs.test.result == 'success' && needs.security-scan.result == 'success' && needs.integration-test.result == 'success'
      run: |
        echo "üéâ All tests passed! Ready for deployment."

    - name: Notify on failure
      if: needs.test.result == 'failure' || needs.security-scan.result == 'failure' || needs.integration-test.result == 'failure'
      run: |
        echo "‚ùå Tests failed. Please check the logs."
        exit 1
